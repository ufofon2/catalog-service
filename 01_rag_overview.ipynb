{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ufofon2/catalog-service/blob/main/01_rag_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ycsiv9AiPAG"
      },
      "source": [
        "## RAG이란 무엇인가?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNTmuX3IiPAH"
      },
      "source": [
        "Retrieval augmented generation (RAG)는 생성형 AI 애플리케이션의 대형 언어 모델(LLM)이 작업을 수행할 때 가장 관련 있고 컨텍스트에서 중요한 독점적, 비공개 또는 동적 데이터를 제공하여 정확성과 성능을 향상시키는 아키텍처입니다.\n",
        "\n",
        "생성 AI에서 RAG는 사용자의 쿼리에 맞는 가장 컨텍스트에 관련성이 높은 결과를 데이터베이스에서 가져오는 기술입니다.\n",
        "\n",
        "OpenAI의 ChatGPT와 Anthropic의 Claude와 같은 대형 언어 모델(LLM)을 기반으로 구축된 제품들은 뛰어나지만 몇 가지 단점을 가지고 있습니다:\n",
        "\n",
        "- 정적입니다 - LLM은 \"시간에 고정\"되어 있으며 최신 정보를 포함하지 않습니다. 거대한 훈련 데이터셋을 업데이트하는 것은 현실적이지 않습니다.\n",
        "- 도메인별 지식이 부족합니다 - LLM은 일반화된 작업을 위해 훈련되었기 때문에 회사의 비공개 데이터를 알지 못합니다.\n",
        "- \"블랙 박스\"로 작동합니다 - LLM이 결론에 도달할 때 어떤 소스를 고려했는지 이해하기 어렵습니다.\n",
        "- 비효율적이고 생산 비용이 높습니다 - 파운데이션 모델을 생산하고 배포할 재정적, 인적 자원을 가진 조직은 거의 없습니다.\n",
        "\n",
        "이러한 문제들은 LLM을 활용한 생성형 AI 애플리케이션의 정확성에 영향을 미칩니다. 챗봇 데모보다 더 까다로운 요구 사항을 가진 비즈니스 애플리케이션의 경우, 프롬프트 외에 수정 없이 \"그대로\" 사용된 LLM은 고객이 다음 비행기를 예약하는 데 도움을 주는 것과 같은 컨텍스트에 의존하는 작업에서 성능이 저조할 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jn-odZdiPAH"
      },
      "source": [
        "- LLM은 다음 예와 같이 Volvo XC60에 대한 도메인별 정보가 부족합니다.<br><br>\n",
        "\n",
        "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fffc1bd17088006b246799f2c10872ff68fdcb609-1522x1160.png&w=1920&q=75\" width=600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia4XXKi_iPAH"
      },
      "source": [
        "- 도메인별 독점 데이터를 사용하여 임베딩 모델로 벡터 데이터베이스 생성합니다.<br><br>\n",
        "\n",
        "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9a4dc90a3dd063fabbfa8481ed63faee5e7ea045-1770x776.png&w=1920&q=75\" width=600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhZWVfvSiPAH"
      },
      "source": [
        "- RAG는 LLM이 더 정확한 응답을 생성할 수 있도록 관련 있고 시기적절한 컨텍스트를 검색하기 위해 의미론적 검색을 사용합니다.<br><br>\n",
        "\n",
        "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16d192aac65c918a8bd554fcff022541b49498cb-1999x955.png&w=2048&q=75\" width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjPZ5E3RiPAH"
      },
      "source": [
        "- RAG는 LLM의 컨텍스트 윈도우를 통해 도메인별 정보를 제공함으로써 환각의 가능성을 줄입니다.<br><br>\n",
        "\n",
        "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F74e2ae38fb5abdaec054050bd6c7d8ee8cd2858d-1999x1374.png&w=2048&q=75\" width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM_RHQZXiPAH"
      },
      "source": [
        "## RAG의 주요 컴포넌트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o71fi__iiPAI"
      },
      "source": [
        "### 인덱싱:\n",
        "\n",
        "데이터 소스에서 데이터를 가져와 인덱싱하는 파이프라인으로, 일반적으로 오프라인 작업을 수행합니다.\n",
        "\n",
        "1. 데이터 로드: 먼저 데이터를 로드해야 합니다. 이는 일반적으로 Document Loaders를 사용하여 수행됩니다.\n",
        "2. 텍스트 분리: 텍스트 분리기는 큰 문서를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하거나 모델에 전달할 때 유용합니다. 큰 청크는 검색하기 어렵고 모델의 제한된 컨텍스트 윈도우 크기에 맞지 않기 때문입니다.\n",
        "3. 벡터 저장: 나중에 검색할 수 있도록 분리된 청크를 저장하고 인덱싱할 장소가 필요합니다. 이는 VectorStore와 Embeddings 모델을 사용하여 수행됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRbpWmJNiPAI"
      },
      "source": [
        "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=800>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Qadl0_iPAI"
      },
      "source": [
        "### 검색과 생성\n",
        "\n",
        "4. 벡터 검색: 사용자 입력이 주어지면, Retriever를 사용하여 저장소에서 관련된 청크를 검색합니다.\n",
        "5. 답변 생성: ChatModel / LLM은 질문과 검색된 데이터를 포함하는 프롬프트를 사용하여 답변을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEw-eTKiPAI"
      },
      "source": [
        "<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" width=800>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain_rag_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}